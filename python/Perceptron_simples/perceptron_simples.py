# -*- coding: utf-8 -*-
"""Perceptron_simples.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1syqYt2Mo4xuzFOe3umIIOKECdlBwDzn2
"""

#bibliotecas de aprendizado profundo: TensorFlow e Keras
# https://www.kdnuggets.com/2018/10/simple-neural-network-python.html

import numpy as np

import sys 

stdoutOrigin=sys.stdout 
sys.stdout = open("log.txt", "w")

class NeuralNetwork():
    
    def __init__(self):
        # Propagação para geração de número aleatório 
        np.random.seed(1)
        
        # Cria matriz de pesos dos neurônios de  3x1 (3 linhas e uma coluna) com  com valores de -1 a 1 e média de 0
        #A multiplicação e subtração levam o intervalo de valores que numpy.random.random()produz para produzir um intervalo mais amplo.
        #random() sempre produz valores de ponto flutuante entre 0 e 1 e, ao multiplicá-los por 2, subtrair -1 significa que agora você tem valores entre -1 e 1.
        #https://stackoverflow.com/questions/56315667/why-do-we-multiply-calls-to-numpy-random-random-by-numbers-and-subtract-numbers
        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1 # números aleatórios para garantir sua distribuição eficiente.


    def sigmoid(self, x):
        # Aplicando a função sigmóide que desenha uma curva característica em forma de “S”, como uma função de ativação da rede neural.
        # Esta função pode mapear qualquer valor para um valor de 0 a 1. Ela nos ajudará a normalizar a soma ponderada das entradas
        # Binarizar entre 0 e 1 . Ex. classificar um pessoa boa pagadora ou mal pagadora
        # Forma de converter numeros negativos em 0 e positivo em 1
        # não é mais recomendado utilizar a função logística como não linearidade de ativação nas redes neurais artificiais
        #a função Tangente Hiperbólica (TanH), uma alternativa mais atraente do que a sigmoide para servir de ativação às camadas ocultas das RNAs
        # número de Euler, aproximadamente 2.718281, Este valor tem uma relação matemática próxima com pi
        # np.exp()calcula e^xpara cada valor de xem sua matriz de entrada.
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        # Calcular a derivada da  função sigmóide, afim de saber a quantidade apropriada para ajustar (aumentar/reduzir) os pesos (retropropagação).
        # Ajustar os pesos/atualizar de volta
        #derivada da função sigmoide é sempre <1 - https://matheusfacure.github.io/2017/07/12/activ-func/#:~:text=Mais%20ainda%2C%20repare%20que%20a,fun%C3%A7%C3%A3o%20sigmoide%20%C3%A9%20sempre%20%3C1.&text=Assim%2C%20n%C3%A3o%20%C3%A9%20mais%20recomendado,RNA%2C%20para%20modelar%20vari%C3%A1veis%20bin%C3%A1rias.
        return x * (1 - x)

    def train(self, training_inputs, training_outputs, training_iterations):
        
        # treinar o modelo para fazer previsões precisas enquanto ajusta os pesos continuamente
        for iteration in range(training_iterations):
            #siphon the training data via  the neuron
            output = self.think(training_inputs)
            
            print(iteration," Entrada: ********************************************************")
            print(training_inputs, "\n Saida calculada = Sigmoid(entrada * peso)\n ",output,"\n")

            # Calculando a taxa de erro para back-propagation
            error = training_outputs - output
            print("Taxa de erro = (Saida calculada - Saida esperada): \nSaida esperada\n",training_outputs,"\nErro:\n",error,"\n")
            
            # Realizando ajustes de peso
            print("\nDerivada da Sigmoid da soma ponderada (entrada * peso): \n",self.sigmoid_derivative(output),"\n")
            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))
            print("\n matriz de erro x a derivada da sigmoid\n", error * self.sigmoid_derivative(output))
            print("\nAjuste dos pesos = transposta da matriz de entrada x (erro x derivada da Sigmoid):\n",adjustments,"\n")

            self.synaptic_weights += adjustments
            print("\nPesos atualizados: \n",adjustments, "\n")

    def think(self, inputs): #função pensar()
        # Passando as entradas através do neurônio para obter a saída
        # Convertendo valores em flutuantes
        
        inputs = inputs.astype(float)
        # np.dot - função para realizar o produto escalar de duas matrizes, vai multiplcar entrada x peso
        # a = np.dot ( 6 , 12 ) 
        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))
        print("\n=Think=\nPesos\n",self.synaptic_weights,"\nEntrada\n",inputs,"\nnp.dot\n",np.dot(inputs, self.synaptic_weights),"\nsigmoid\n",self.sigmoid(np.dot(inputs, self.synaptic_weights)))
        return output

# Disseminar código - função think
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

x=np.dot([0,0,1], [-0.16595599,0.44064899,-0.99977125])
print("x= ",x)
print(sigmoid(x))

training_inputs

training_inputs.T

# Disseminar código
# Com a reinicialização do seed (todas as vezes), o mesmo conjunto de números aparecerá todas as vezes
np.random.seed(1)
np.random.rand(4)

# Validando... seed(1) - abaixo gerará os mesmos números aletórios
np.random.seed(1)
np.random.rand(4)

synaptic_weights = 2 * np.random.random((3, 1)) - 1
synaptic_weights

# M A I N *******************************************************************************

#initializing the neuron class
neural_network = NeuralNetwork()

print("Pesos iniciais gerados aleatoriamente: ")
print(neural_network.synaptic_weights,"\n")

# Dados de treinamento consistindo em 4 exemplos. Sendo 3 valores de entrada 
training_inputs = np.array([[0,0,1],
                            [1,1,1],
                            [1,0,1],
                            [0,1,1]])

# TREINAMENTO SUPERVIDIONADO
# 'T' matriz transposta. O que é linha vira coluna "for transposing the matrix from horizontal position to vertical position"
# valores estimado de saída
training_outputs = np.array([[0,1,1,0]]).T 
training_outputs 

# OBS- multiplicar matrizes a A e B, a B tem que ter a quantidade de linha

# Dissecando

b=np.array([[0,1,1,0]]) # sem "T". matriz transposta 
print(b)
print(b.size)

# T R E I N A M E N T O  ==========================================================
neural_network.train(training_inputs, training_outputs, 15000)

#==== fechar arquivo de log  ======================================================
sys.stdout.close()
sys.stdout=stdoutOrigin

print("\nValores do peso após o treinamento: \n")
print(neural_network.synaptic_weights)

user_input_one = str(input("Digite o Valor de entrada 1: "))
user_input_two = str(input("Digite o Valor de entrada 2: "))
user_input_three = str(input("Digite o Valor de entrada 3: "))

print("Nova entrada: ", user_input_one, user_input_two, user_input_three)
print("Novos dados de saída: ")
print(neural_network.think(np.array([user_input_one, user_input_two, user_input_three])))



outro exemplo =================================

# https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6
class NeuralNetwork2:
    def __init__(self, x, y):
        self.input      = x
        # 2 camadas
        self.weights1   = np.random.rand(self.input.shape[1],4) 
        self.weights2   = np.random.rand(4,1)                 
        self.y          = y
        self.output     = np.zeros(self.y.shape)

    def feedforward(self):
        self.layer1 = sigmoid(np.dot(self.input, self.weights1))
        self.output = sigmoid(np.dot(self.layer1, self.weights2))

    def backprop(self):
        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1
        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))
        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))

        # update the weights with the derivative (slope) of the loss function
        self.weights1 += d_weights1
        self.weights2 += d_weights2